"""
ë‚˜ë¹„ì–Œ ë©”ì¸ ì±—ë´‡ í´ë˜ìŠ¤
ì „ì²´ ëŒ€í™” í”Œë¡œìš° ê´€ë¦¬ ë° ëª¨ë“ˆ í†µí•©
"""

import time
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import asdict
import json
from pathlib import Path

from data.data_structure import (
    UserInput, ChatbotOutput, ChatbotResponse, ExtractedInfo, ExtractedEntity,
    UserProfile, NaviyamKnowledge, IntentType, ConfidenceLevel, UserState, LearningData
)
from data.data_loader import NaviyamDataLoader
from models.model_factory import create_model, ModelSelection
from models.models_config import ModelConfigManager
from nlp.preprocessor import NaviyamTextPreprocessor, EmotionType
from nlp.nlu import NaviyamNLU
from nlp.nlg import NaviyamNLG, ResponseTone
from nlp.llm_normalizer import LLMNormalizer, LLMNormalizedOutput
from .user_manager import NaviyamUserManager
from .response_generator import NaviyamResponseGenerator
from rag.retriever import create_naviyam_retriever

logger = logging.getLogger(__name__)


class ConversationMemory:
    """ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬"""

    def __init__(self, max_history: int = 10):
        self.max_history = max_history
        self.conversations = {}  # user_id -> conversation_history

    def add_conversation(self, user_id: str, user_input: str, bot_response: str, extracted_info: ExtractedInfo):
        """ëŒ€í™” ì¶”ê°€"""
        if user_id not in self.conversations:
            self.conversations[user_id] = []

        conversation_item = {
            "timestamp": datetime.now().isoformat(),
            "user_input": user_input,
            "bot_response": bot_response,
            "intent": extracted_info.intent.value,
            "confidence": extracted_info.confidence,
            "entities": asdict(extracted_info.entities)
        }

        self.conversations[user_id].append(conversation_item)

        # ìµœëŒ€ íˆìŠ¤í† ë¦¬ ìœ ì§€
        if len(self.conversations[user_id]) > self.max_history:
            self.conversations[user_id] = self.conversations[user_id][-self.max_history:]

    def get_recent_conversations(self, user_id: str, count: int = 3) -> List[Dict]:
        """ìµœê·¼ ëŒ€í™” ì¡°íšŒ"""
        if user_id not in self.conversations:
            return []
        return self.conversations[user_id][-count:]

    def clear_conversations(self, user_id: str = None):
        """ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
        if user_id:
            self.conversations.pop(user_id, None)
        else:
            self.conversations.clear()


class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""

    def __init__(self):
        self.metrics = {
            "total_conversations": 0,
            "total_response_time": 0.0,
            "avg_response_time": 0.0,
            "intent_accuracy": 0.0,
            "user_satisfaction": 0.0,
            "error_count": 0,
            "successful_recommendations": 0
        }
        self.response_times = []

    def record_conversation(self, response_time: float, success: bool = True):
        """ëŒ€í™” ê¸°ë¡"""
        self.metrics["total_conversations"] += 1
        self.metrics["total_response_time"] += response_time
        self.response_times.append(response_time)

        if success:
            self.metrics["successful_recommendations"] += 1
        else:
            self.metrics["error_count"] += 1

        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        if self.metrics["total_conversations"] > 0:
            self.metrics["avg_response_time"] = (
                    self.metrics["total_response_time"] / self.metrics["total_conversations"]
            )

    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ë°˜í™˜"""
        recent_times = self.response_times[-100:] if len(self.response_times) > 100 else self.response_times

        summary = self.metrics.copy()
        if recent_times:
            summary["recent_avg_response_time"] = sum(recent_times) / len(recent_times)
            summary["recent_max_response_time"] = max(recent_times)
            summary["recent_min_response_time"] = min(recent_times)

        summary["success_rate"] = (
                self.metrics["successful_recommendations"] / max(self.metrics["total_conversations"], 1)
        )

        return summary


class NaviyamChatbot:
    """ë‚˜ë¹„ì–Œ ë©”ì¸ ì±—ë´‡"""

    def __init__(self, config):
        """
        Args:
            config: AppConfig ê°ì²´
        """
        self.config = config

        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤
        self.knowledge: Optional[NaviyamKnowledge] = None
        self.model = None  # A.X 3.1 Lite ë˜ëŠ” KoAlpaca ëª¨ë¸
        self.preprocessor: Optional[NaviyamTextPreprocessor] = None
        self.nlu: Optional[NaviyamNLU] = None
        self.nlg: Optional[NaviyamNLG] = None
        self.user_manager: Optional[NaviyamUserManager] = None
        self.response_generator: Optional[NaviyamResponseGenerator] = None
        self.llm_normalizer: Optional[LLMNormalizer] = None
        self.data_collector = None

        # ë©”ëª¨ë¦¬ ë° ëª¨ë‹ˆí„°ë§
        self.conversation_memory = ConversationMemory(config.data.max_conversations)
        self.performance_monitor = PerformanceMonitor()

        # ìƒíƒœ ê´€ë¦¬
        self.is_initialized = False
        self.last_cleanup_time = datetime.now()

        # ì´ˆê¸°í™”
        self._initialize_components()

    def _initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ìƒì„¸ ì„±ëŠ¥ ì¸¡ì • í¬í•¨)"""
        try:
            logger.info("ë‚˜ë¹„ì–Œ ì±—ë´‡ ì´ˆê¸°í™” ì‹œì‘...")
            total_start = time.time()

            # 1. ë°ì´í„° ë¡œë“œ
            step_start = time.time()
            self._load_knowledge_base()
            kb_time = time.time() - step_start
            logger.info(f"[ì¸¡ì •] ì§€ì‹ë² ì´ìŠ¤ ë¡œë“œ: {kb_time:.3f}ì´ˆ")

            # 2. ëª¨ë¸ ë¡œë“œ (ì„ íƒì )
            if self.config.model and hasattr(self.config.model, 'model_name'):
                step_start = time.time()
                self._load_language_model()
                llm_time = time.time() - step_start
                logger.info(f"[ì¸¡ì •] LLM ëª¨ë¸ ë¡œë“œ: {llm_time:.3f}ì´ˆ")
            else:
                llm_time = 0.0

            # 3. NLP ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            step_start = time.time()
            self._initialize_nlp_components()
            nlp_time = time.time() - step_start
            logger.info(f"[ì¸¡ì •] NLP ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”: {nlp_time:.3f}ì´ˆ")

            # 4. ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™”
            step_start = time.time()
            self._initialize_response_components()
            response_time = time.time() - step_start
            logger.info(f"[ì¸¡ì •] ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™”: {response_time:.3f}ì´ˆ")

            # 5. ì‚¬ìš©ì ê´€ë¦¬ì ì´ˆê¸°í™”
            step_start = time.time()
            self._initialize_user_manager()
            user_mgr_time = time.time() - step_start
            logger.info(f"[ì¸¡ì •] ì‚¬ìš©ì ê´€ë¦¬ì ì´ˆê¸°í™”: {user_mgr_time:.3f}ì´ˆ")

            # 6. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            step_start = time.time()
            self._initialize_rag_system()
            rag_time = time.time() - step_start
            logger.info(f"[ì¸¡ì •] RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”: {rag_time:.3f}ì´ˆ")

            total_time = time.time() - total_start
            
            # ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸
            logger.info("=" * 50)
            logger.info("ğŸš€ Phase 3 ì„±ëŠ¥ ì¸¡ì • ë¦¬í¬íŠ¸")
            logger.info("=" * 50)
            logger.info(f"ì´ ì´ˆê¸°í™” ì‹œê°„: {total_time:.3f}ì´ˆ")
            logger.info(f"ğŸ“Š ì»´í¬ë„ŒíŠ¸ë³„ ì†Œìš” ì‹œê°„:")
            logger.info(f"  â€¢ LLM ëª¨ë¸ ë¡œë“œ:     {llm_time:.3f}ì´ˆ ({llm_time/total_time*100:.1f}%)")
            logger.info(f"  â€¢ RAG ì‹œìŠ¤í…œ:       {rag_time:.3f}ì´ˆ ({rag_time/total_time*100:.1f}%)")
            logger.info(f"  â€¢ ì§€ì‹ë² ì´ìŠ¤:       {kb_time:.3f}ì´ˆ ({kb_time/total_time*100:.1f}%)")
            logger.info(f"  â€¢ NLP ì»´í¬ë„ŒíŠ¸:     {nlp_time:.3f}ì´ˆ ({nlp_time/total_time*100:.1f}%)")
            logger.info(f"  â€¢ ì‘ë‹µ ìƒì„±ê¸°:       {response_time:.3f}ì´ˆ ({response_time/total_time*100:.1f}%)")
            logger.info(f"  â€¢ ì‚¬ìš©ì ê´€ë¦¬ì:     {user_mgr_time:.3f}ì´ˆ ({user_mgr_time/total_time*100:.1f}%)")
            logger.info("=" * 50)
            
            if total_time > 3.0:
                logger.warning(f"âš ï¸  3ì´ˆ ëª©í‘œ ë¯¸ë‹¬ì„±! {total_time-3.0:.3f}ì´ˆ ì¶”ê°€ ìµœì í™” í•„ìš”")
            else:
                logger.info(f"âœ… 3ì´ˆ ëª©í‘œ ë‹¬ì„±! ({3.0-total_time:.3f}ì´ˆ ì—¬ìœ )")

            self.is_initialized = True

        except Exception as e:
            logger.error(f"ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"ì±—ë´‡ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def _load_knowledge_base(self):
        """ì§€ì‹ë² ì´ìŠ¤ ë¡œë“œ"""
        logger.info("ë‚˜ë¹„ì–Œ ì§€ì‹ë² ì´ìŠ¤ ë¡œë”©...")

        try:
            data_loader = NaviyamDataLoader(self.config.data, self.config.debug)
            self.knowledge = data_loader.load_all_data()

            logger.info(f"ì§€ì‹ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ: "
                        f"ê°€ê²Œ {len(self.knowledge.shops)}ê°œ, "
                        f"ë©”ë‰´ {len(self.knowledge.menus)}ê°œ")

        except Exception as e:
            logger.error(f"ì§€ì‹ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ì§€ì‹ë² ì´ìŠ¤ë¡œ í´ë°±
            self.knowledge = NaviyamKnowledge()

    def _load_language_model(self):
        """ì–¸ì–´ ëª¨ë¸ ë¡œë“œ (A.X 3.1 Lite ë˜ëŠ” KoAlpaca)"""
        if not self.config.model:
            return

        try:
            # ModelConfigì—ì„œ model_type í™•ì¸
            model_type = getattr(self.config.model, 'model_type', 'ax')
            model_name = "A.X 3.1 Lite" if model_type == 'ax' else "KoAlpaca"
            
            logger.info(f"{model_name} ëª¨ë¸ ë¡œë”©...")

            # ëª¨ë¸ íŒ©í† ë¦¬ë¡œ ëª¨ë¸ ìƒì„±
            self.model = create_model(
                model_config=self.config.model,
                model_type=model_type,
                cache_dir=self.config.data.cache_dir
            )

            logger.info(f"{model_name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        except Exception as e:
            logger.warning(f"ì–¸ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. í…œí”Œë¦¿ ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
            self.model = None

    def _initialize_nlp_components(self):
        """NLP ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        logger.info("NLP ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”...")

        self.preprocessor = NaviyamTextPreprocessor(preserve_expressions=True)
        self.nlu = NaviyamNLU(use_preprocessor=True)
        self.nlg = NaviyamNLG(default_tone=ResponseTone.FRIENDLY)

        if self.model:
            self.llm_normalizer = LLMNormalizer(self.model)
            logger.info("LLM ì •ê·œí™”ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

        logger.info("NLP ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_response_components(self):
        """ì‘ë‹µ ìƒì„± ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        logger.info("ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™”...")

        self.response_generator = NaviyamResponseGenerator(
            knowledge=self.knowledge,
            nlg=self.nlg,
            model=self.model
        )

        logger.info("ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_user_manager(self):
        """ì‚¬ìš©ì ê´€ë¦¬ì ì´ˆê¸°í™”"""
        logger.info("ì‚¬ìš©ì ê´€ë¦¬ì ì´ˆê¸°í™”...")

        self.user_manager = NaviyamUserManager(
            save_path=str(Path(self.config.data.output_path) / "user_profiles"),
            enable_personalization=self.config.inference.enable_personalization
        )

        logger.info("ì‚¬ìš©ì ê´€ë¦¬ì ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_rag_system(self):
        """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
        
        try:
            # ì„¤ì •ì—ì„œ Vector Store íƒ€ì… ì½ê¸°
            vector_store_type = getattr(self.config, 'rag', None)
            if vector_store_type and hasattr(vector_store_type, 'vector_store_type'):
                store_type = vector_store_type.vector_store_type
                index_path = getattr(vector_store_type, 'index_path', None)
            else:
                store_type = "mock"  # ê¸°ë³¸ê°’
                index_path = None
            
            logger.info(f"RAG Vector Store íƒ€ì…: {store_type}")
            
            # RAG Retriever ìƒì„± (test_data.json ì‚¬ìš©)
            if store_type == "faiss" and index_path:
                # FAISS ì „ìš© ìƒì„± ë¡œì§
                from rag.vector_stores import FAISSVectorStore
                from rag.query_parser import QueryStructurizer
                from rag.retriever import NaviyamRetriever, load_knowledge_from_file
                
                # Vector Store ìƒì„±
                faiss_store = FAISSVectorStore(
                    embedding_model=None,
                    index_path=index_path
                )
                
                # Query Structurizer ìƒì„±
                query_structurizer = QueryStructurizer(llm_client=None)
                
                # Retriever ìƒì„±
                self.retriever = NaviyamRetriever(faiss_store, query_structurizer)
                
                # ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ë° ì¶”ê°€
                knowledge_data = load_knowledge_from_file("rag/test_data.json")
                if knowledge_data:
                    self.retriever.add_knowledge_base(knowledge_data)
                    
                logger.info("FAISS RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                # Mock ë˜ëŠ” ê¸°íƒ€ íƒ€ì…
                self.retriever = create_naviyam_retriever(
                    knowledge_file_path="rag/test_data.json",
                    vector_store_type=store_type
                )
                logger.info(f"{store_type} RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # RAG ì‹¤íŒ¨ ì‹œì—ë„ ì±—ë´‡ì´ ë™ì‘í•˜ë„ë¡ Noneìœ¼ë¡œ ì„¤ì •
            self.retriever = None
            logger.warning("RAG ì—†ì´ ì±—ë´‡ ì‹¤í–‰")

    def process_user_input(self, user_input: UserInput) -> ChatbotOutput:
        """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (ë©”ì¸ ë©”ì„œë“œ)"""
        if not self.is_initialized:
            raise RuntimeError("ì±—ë´‡ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        start_time = time.time()

        try:
            # 1. ì…ë ¥ ê²€ì¦
            if not user_input.text or not user_input.text.strip():
                return self._generate_empty_input_response(user_input)

            # 2. ì „ì²˜ë¦¬
            preprocessed = self.preprocessor.preprocess(user_input.text)

            # 3. ì˜ë„ ë° ì—”í‹°í‹° ì¶”ì¶œ
            # extracted_info = self.nlu.extract_intent_and_entities(
            #     user_input.text, user_input.user_id
            # )
            # 3. ìŠ¤ë§ˆíŠ¸ NLU ì²˜ë¦¬ (LLM í†µí•©)
            extracted_info = self._smart_nlu_processing(user_input, preprocessed)

            # 3.5. RAG ê²€ìƒ‰ (ì¶”ì²œ ê´€ë ¨ ì˜ë„ì¸ ê²½ìš°)
            rag_context = self._perform_rag_search(user_input, extracted_info)

            # 4. ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ/ì—…ë°ì´íŠ¸
            user_profile = self.user_manager.get_or_create_user_profile(user_input.user_id)
            self.user_manager.update_user_interaction(
                user_input.user_id, extracted_info, preprocessed.emotion
            )

            # 5. ì‘ë‹µ ìƒì„±
            # response = self.response_generator.generate_response(
            #     extracted_info=extracted_info,
            #     user_profile=user_profile,
            #     conversation_context=self.conversation_memory.get_recent_conversations(
            #         user_input.user_id, 3
            #     )
            # )

            # 5. ìŠ¤ë§ˆíŠ¸ ì‘ë‹µ ìƒì„± (LLM í†µí•©)
            response = self._smart_response_generation(
                extracted_info, user_profile, user_input.user_id, rag_context
            )
            if response.metadata.get("onboarding_complete"):
                # ì‚¬ìš©ìë¥¼ normal_modeë¡œ ì „í™˜í•˜ê¸° ìœ„í•´ interaction_count ì¦ê°€
                if user_profile:
                    user_profile.interaction_count = max(user_profile.interaction_count, 3)
                    user_profile.data_completeness = 1.0  # ì˜¨ë³´ë”© ì™„ë£Œë¡œ ì„¤ì •
                    self.user_manager._save_user_profile(user_profile)

                logger.info(f"ì‚¬ìš©ì {user_input.user_id} ì˜¨ë³´ë”© ì™„ë£Œ")
            # 6. ê°œì¸í™” ì ìš©
            response = self.user_manager.personalize_response(response, user_profile)

            # 7. í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
            learning_data = self._collect_learning_data(
                user_input, extracted_info, response, preprocessed
            )

            # 8. ë°ì´í„° ìˆ˜ì§‘ê¸°ì— ì „ë‹¬ (ìƒˆë¡œ ì¶”ê°€)
            if self.data_collector:
                self.data_collector.collect_interaction_data(user_input.user_id, learning_data)

                # ì¶”ì²œ ë°ì´í„°ë„ ìˆ˜ì§‘
                if response.recommendations:
                    self.data_collector.collect_recommendation_data(
                        user_id=user_input.user_id,
                        recommendations=response.recommendations,
                        user_selection=None  # ë‚˜ì¤‘ì— ì‚¬ìš©ì ì„ íƒì‹œ ì—…ë°ì´íŠ¸
                    )

            # 8. ì„¸ì…˜ ë°ì´í„° ìƒì„±
            session_data = self._generate_session_data(
                user_input, extracted_info, response
            )

            # 9. ëŒ€í™” ê¸°ë¡ ì €ì¥
            if self.config.inference.save_conversations:
                self.conversation_memory.add_conversation(
                    user_input.user_id, user_input.text, response.text, extracted_info
                )

            # 10. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            response_time = time.time() - start_time
            self.performance_monitor.record_conversation(response_time, True)

            # 11. ë©”ëª¨ë¦¬ ì •ë¦¬ (ì£¼ê¸°ì )
            self._periodic_cleanup()

            return ChatbotOutput(
                response=response,
                extracted_info=extracted_info,
                learning_data=learning_data,
                session_data=session_data
            )

        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

            # ì—ëŸ¬ ì‘ë‹µ ìƒì„±
            error_response = self._generate_error_response(user_input, str(e))

            response_time = time.time() - start_time
            self.performance_monitor.record_conversation(response_time, False)

            return error_response

    def _generate_empty_input_response(self, user_input: UserInput) -> ChatbotOutput:
        """ë¹ˆ ì…ë ¥ ì‘ë‹µ ìƒì„±"""
        response = ChatbotResponse(
            text="ì•ˆë…•í•˜ì„¸ìš”! ë­ ë“œì‹œê³  ì‹¶ì€ì§€ ì•Œë ¤ì£¼ì„¸ìš”! ğŸ˜Š",
            recommendations=[],
            follow_up_questions=["ì–´ë–¤ ìŒì‹ì´ ë“œì‹œê³  ì‹¶ìœ¼ì„¸ìš”?", "ì˜ˆì‚°ì€ ì–¼ë§ˆë‚˜ ìƒê°í•˜ê³  ê³„ì„¸ìš”?"],
            action_required=False
        )

        return ChatbotOutput(
            response=response,
            extracted_info=ExtractedInfo(
                intent=IntentType.GENERAL_CHAT,
                entities=ExtractedEntity(),
                confidence=1.0,
                raw_text="",
                confidence_level=ConfidenceLevel.HIGH
            ),
            learning_data={},
            session_data={}
        )

    def _generate_error_response(self, user_input: UserInput, error_msg: str) -> ChatbotOutput:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        response = ChatbotResponse(
            text="ì£„ì†¡í•´ìš”! ì ì‹œ ë¬¸ì œê°€ ìˆëŠ” ê²ƒ ê°™ì•„ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”? ğŸ˜…",
            recommendations=[],
            follow_up_questions=["ê°„ë‹¨í•˜ê²Œ ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”!"],
            action_required=False,
            metadata={"error": error_msg}
        )

        return ChatbotOutput(
            response=response,
            extracted_info=ExtractedInfo(
                intent=IntentType.GENERAL_CHAT,
                entities=ExtractedEntity(),
                confidence=0.0,
                raw_text=user_input.text,
                confidence_level=ConfidenceLevel.VERY_LOW
            ),
            learning_data={},
            session_data={"error": True}
        )

    def _collect_learning_data(
            self,
            user_input: UserInput,
            extracted_info: ExtractedInfo,
            response: ChatbotResponse,
            preprocessed: Any
    ) -> Dict[str, Any]:
        """í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘"""
        learning_data = {
            "timestamp": user_input.timestamp.isoformat(),
            "user_id": user_input.user_id,
            "input_text": user_input.text,
            "extracted_intent": extracted_info.intent.value,
            "extracted_entities": asdict(extracted_info.entities) if extracted_info.entities else {},
            "confidence": extracted_info.confidence,
            "response_text": response.text,
            "emotion": preprocessed.emotion.value if preprocessed else "neutral",
            "keywords": preprocessed.extracted_keywords if preprocessed else [],
            "user_strategy": self._determine_user_strategy(user_input.user_id),
            "conversation_turn": len(self.conversation_memory.get_recent_conversations(user_input.user_id)),
            "session_id": user_input.session_id
        }

        # Feature ì¶”ì¶œ
        if extracted_info.entities:
            entities = extracted_info.entities

            # ìŒì‹ ì„ í˜¸ë„
            if entities.food_type:
                learning_data["food_preference_extracted"] = entities.food_type

            # ì˜ˆì‚° íŒ¨í„´
            if entities.budget:
                learning_data["budget_pattern_extracted"] = entities.budget

            # ë™ë°˜ì íŒ¨í„´
            if entities.companions:
                learning_data["companion_pattern_extracted"] = entities.companions

            # ìœ„ì¹˜ ì„ í˜¸ë„
            if entities.location_preference:
                learning_data["location_preference_extracted"] = entities.location_preference

        # ì¶”ì²œ ê´€ë ¨ ë°ì´í„°
        if response.recommendations:
            learning_data["recommendation_provided"] = True
            learning_data["recommendation_count"] = len(response.recommendations)
            learning_data["recommendations"] = response.recommendations
        else:
            learning_data["recommendation_provided"] = False

        return learning_data

    def _determine_user_strategy(self, user_id: str) -> str:
        """ì‚¬ìš©ì ì „ëµ ê²°ì • (ê°„ë‹¨ ë²„ì „)"""
        if self.user_manager:
            return self.user_manager.determine_user_strategy(user_id)
        return "normal_mode"

    def _generate_session_data(
            self,
            user_input: UserInput,
            extracted_info: ExtractedInfo,
            response: ChatbotResponse
    ) -> Dict[str, Any]:
        """ì„¸ì…˜ ë°ì´í„° ìƒì„±"""
        return {
            "session_id": user_input.session_id,
            "conversation_turn": len(self.conversation_memory.get_recent_conversations(user_input.user_id)) + 1,
            "intent_confidence": extracted_info.confidence,
            "response_length": len(response.text),
            "has_recommendations": len(response.recommendations) > 0,
            "has_follow_ups": len(response.follow_up_questions) > 0,
            "processing_timestamp": datetime.now().isoformat()
        }

    def _periodic_cleanup(self):
        """ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬"""
        current_time = datetime.now()

        # 1ì‹œê°„ë§ˆë‹¤ ì •ë¦¬
        if current_time - self.last_cleanup_time > timedelta(hours=1):
            logger.info("ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤í–‰...")

            # ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬
            if self.model:
                self.model.cleanup_memory()

            # ì˜¤ë˜ëœ ëŒ€í™” ê¸°ë¡ ì •ë¦¬ (24ì‹œê°„ ì´ìƒ)
            self._cleanup_old_conversations()

            self.last_cleanup_time = current_time
            logger.info("ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

    def _cleanup_old_conversations(self):
        """ì˜¤ë˜ëœ ëŒ€í™” ê¸°ë¡ ì •ë¦¬"""
        cutoff_time = datetime.now() - timedelta(hours=24)

        for user_id in list(self.conversation_memory.conversations.keys()):
            conversations = self.conversation_memory.conversations[user_id]
            recent_conversations = []

            for conv in conversations:
                conv_time = datetime.fromisoformat(conv["timestamp"])
                if conv_time > cutoff_time:
                    recent_conversations.append(conv)

            if recent_conversations:
                self.conversation_memory.conversations[user_id] = recent_conversations
            else:
                del self.conversation_memory.conversations[user_id]

    def chat(self, message: str, user_id: str = "default_user") -> str:
        """ê°„ë‹¨í•œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
        user_input = UserInput(
            text=message,
            user_id=user_id,
            timestamp=datetime.now()
        )

        output = self.process_user_input(user_input)
        return output.response.text

    def get_conversation_history(self, user_id: str, count: int = 10) -> List[Dict]:
        """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
        return self.conversation_memory.get_recent_conversations(user_id, count)

    def get_user_profile(self, user_id: str) -> Optional[UserProfile]:
        """ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ"""
        if not self.user_manager:
            return None
        return self.user_manager.get_user_profile(user_id)

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì§€í‘œ ì¡°íšŒ"""
        metrics = self.performance_monitor.get_performance_summary()

        # ì¶”ê°€ ì‹œìŠ¤í…œ ì •ë³´
        if self.model:
            model_info = self.model.get_model_info()
            metrics.update(model_info)

        metrics["knowledge_base_size"] = {
            "shops": len(self.knowledge.shops) if self.knowledge else 0,
            "menus": len(self.knowledge.menus) if self.knowledge else 0,
            "coupons": len(self.knowledge.coupons) if self.knowledge else 0
        }

        return metrics

    def reset_conversation(self, user_id: str = None):
        """ëŒ€í™” ë¦¬ì…‹"""
        if user_id:
            self.conversation_memory.clear_conversations(user_id)
            if self.nlu:
                self.nlu.clear_context(user_id)
            logger.info(f"ì‚¬ìš©ì {user_id} ëŒ€í™” ë¦¬ì…‹")
        else:
            self.conversation_memory.clear_conversations()
            if self.nlu:
                self.nlu.clear_context()
            logger.info("ëª¨ë“  ëŒ€í™” ë¦¬ì…‹")

    def save_state(self, file_path: str):
        """ì±—ë´‡ ìƒíƒœ ì €ì¥"""
        state = {
            "conversation_memory": self.conversation_memory.conversations,
            "performance_metrics": self.performance_monitor.metrics,
            "timestamp": datetime.now().isoformat()
        }

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)

        logger.info(f"ì±—ë´‡ ìƒíƒœ ì €ì¥: {file_path}")

    def load_state(self, file_path: str):
        """ì±—ë´‡ ìƒíƒœ ë¡œë“œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                state = json.load(f)

            self.conversation_memory.conversations = state.get("conversation_memory", {})
            self.performance_monitor.metrics.update(state.get("performance_metrics", {}))

            logger.info(f"ì±—ë´‡ ìƒíƒœ ë¡œë“œ: {file_path}")

        except Exception as e:
            logger.warning(f"ì±—ë´‡ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def __del__(self):
        """ì†Œë©¸ì"""
        try:
            if self.model:
                self.model.cleanup_memory()
        except:
            pass

    def _smart_nlu_processing(self, user_input: UserInput, preprocessed) -> ExtractedInfo:
        """ìŠ¤ë§ˆíŠ¸ NLU ì²˜ë¦¬ (LLM í†µí•©)"""

        # LLM ì •ê·œí™” ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        if (self.llm_normalizer and
                self.llm_normalizer.should_use_llm_normalization(user_input.text)):

            logger.debug(f"ë³µì¡í•œ ì…ë ¥ ê°ì§€, LLM ì •ê·œí™” ì‚¬ìš©: {user_input.text}")

            # ëŒ€í™” ë§¥ë½ ìˆ˜ì§‘
            conversation_context = self.conversation_memory.get_recent_conversations(
                user_input.user_id, 3
            )

            # ì‚¬ìš©ì ë§¥ë½ ìˆ˜ì§‘
            user_profile = self.user_manager.get_user_profile(user_input.user_id)
            user_context = {}
            if user_profile:
                user_context = {
                    "preferred_foods": user_profile.preferred_categories,
                    "usual_budget": user_profile.average_budget
                }

            # LLMìœ¼ë¡œ ì…ë ¥ ì •ê·œí™”
            llm_output = self.llm_normalizer.normalize_user_input(
                user_input.text,
                conversation_context,
                user_context
            )

            # LLM ì •ê·œí™” ê²°ê³¼ë¡œ NLU ìˆ˜í–‰
            extracted_info = self.nlu.extract_from_llm_normalized(
                user_input.text, llm_output, user_input.user_id
            )

            logger.debug(f"LLM+NLU ê²°ê³¼: intent={extracted_info.intent.value}, confidence={extracted_info.confidence}")

        else:
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ NLU ìˆ˜í–‰
            extracted_info = self.nlu.extract_intent_and_entities(
                user_input.text, user_input.user_id
            )
            logger.debug(f"ê¸°ì¡´ NLU ê²°ê³¼: intent={extracted_info.intent.value}, confidence={extracted_info.confidence}")

        return extracted_info

    def _perform_rag_search(self, user_input: UserInput, extracted_info: ExtractedInfo) -> str:
        """RAG ê²€ìƒ‰ ìˆ˜í–‰"""
        if not self.retriever:
            return ""
        
        # ì¶”ì²œ ê´€ë ¨ ì˜ë„ì—ì„œë§Œ RAG ê²€ìƒ‰ ìˆ˜í–‰
        recommendation_intents = [
            IntentType.FOOD_REQUEST,  # ìŒì‹ ì¶”ì²œ ìš”ì²­
            IntentType.LOCATION_INQUIRY,  # ê°€ê²Œ ìœ„ì¹˜ ë¬¸ì˜
            IntentType.MENU_OPTION  # ë©”ë‰´ ê´€ë ¨ ë¬¸ì˜
        ]
        
        if extracted_info.intent not in recommendation_intents:
            return ""
        
        try:
            # RAG ê²€ìƒ‰ ìˆ˜í–‰
            rag_context = self.retriever.get_context_for_llm(user_input.text)
            logger.info(f"RAG ê²€ìƒ‰ ì™„ë£Œ: {len(rag_context)} ë¬¸ì")
            return rag_context
            
        except Exception as e:
            logger.error(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return ""

    def _smart_response_generation(
            self,
            extracted_info: ExtractedInfo,
            user_profile,
            user_id: str,
            rag_context: str = ""
    ) -> ChatbotResponse:
        """ìŠ¤ë§ˆíŠ¸ ì‘ë‹µ ìƒì„± (LLM í†µí•©)"""

        # ëŒ€í™” ë§¥ë½ ìˆ˜ì§‘
        conversation_context = self.conversation_memory.get_recent_conversations(user_id, 3)

        user_strategy = self.user_manager.determine_user_strategy(user_id)

        # ì˜¨ë³´ë”© ëª¨ë“œë©´ ë¬´ì¡°ê±´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        if user_strategy == "onboarding_mode":
            logger.debug("ì˜¨ë³´ë”© ëª¨ë“œ: í…œí”Œë¦¿ ì‘ë‹µ ì‚¬ìš©")
            return self.response_generator.generate_response(
                extracted_info=extracted_info,
                user_profile=user_profile,
                conversation_context=conversation_context,
                rag_context=rag_context
            )

        # ì°½ì˜ì  LLM ì‘ë‹µì´ í•„ìš”í•œì§€ íŒë‹¨
        if (self.llm_normalizer and
                self.llm_normalizer.should_use_llm_response(extracted_info, conversation_context)):

            logger.debug("ì°½ì˜ì  LLM ì‘ë‹µ ìƒì„± ì‹œë„")

            # ë¨¼ì € ê¸°ë³¸ ì¶”ì²œ ë°ì´í„° ìƒì„±
            base_response = self.response_generator.generate_response(
                extracted_info=extracted_info,
                user_profile=user_profile,
                conversation_context=conversation_context,
                rag_context=rag_context
            )

            # LLMìœ¼ë¡œ ì•„ë™ ì¹œí™”ì  ì‘ë‹µ ìƒì„±
            llm_response_text = self.llm_normalizer.generate_child_friendly_response(
                extracted_info, base_response.recommendations, conversation_context
            )

            # LLM ì‘ë‹µì´ ì„±ê³µì ì´ë©´ ì‚¬ìš©
            if llm_response_text and len(llm_response_text.strip()) > 10:
                logger.debug("LLM ì‘ë‹µ ìƒì„± ì„±ê³µ")
                base_response.text = llm_response_text
                base_response.metadata["generation_method"] = "llm_child_friendly"
            else:
                logger.debug("LLM ì‘ë‹µ ìƒì„± ì‹¤íŒ¨, í…œí”Œë¦¿ ì‚¬ìš©")
                base_response.metadata["generation_method"] = "template_fallback"

            return base_response

        else:
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‘ë‹µ ìƒì„±
            logger.debug("ê¸°ì¡´ í…œí”Œë¦¿ ì‘ë‹µ ìƒì„±")
            response = self.response_generator.generate_response(
                extracted_info=extracted_info,
                user_profile=user_profile,
                conversation_context=conversation_context,
                rag_context=rag_context
            )
            response.metadata["generation_method"] = "template"
            return response


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_naviyam_chatbot(config) -> NaviyamChatbot:
    """ë‚˜ë¹„ì–Œ ì±—ë´‡ ìƒì„± (í¸ì˜ í•¨ìˆ˜)"""
    return NaviyamChatbot(config)


def quick_chat(message: str, config, user_id: str = "test_user") -> str:
    """ë¹ ë¥¸ ì±„íŒ… (í¸ì˜ í•¨ìˆ˜)"""
    chatbot = NaviyamChatbot(config)
    return chatbot.chat(message, user_id)